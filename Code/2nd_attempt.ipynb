{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6c7e27c",
   "metadata": {},
   "source": [
    "### Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4ce283f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\george\\anaconda3\\envs\\tensor_cuda\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\users\\george\\anaconda3\\envs\\tensor_cuda\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "c:\\users\\george\\anaconda3\\envs\\tensor_cuda\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import glob\n",
    "from joblib import Parallel, delayed # Run functions in parallel\n",
    "# import scipy as sc # \n",
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "from enum import Enum\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46410475",
   "metadata": {},
   "source": [
    "### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0cfcffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../dataset/optiver-realized-volatility-prediction/'\n",
    "\n",
    "use_GPU = True\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "# pd.set_option('max_columns', 300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17b79c3",
   "metadata": {},
   "source": [
    "### Functional Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5814948",
   "metadata": {},
   "outputs": [],
   "source": [
    "buckets_count = 9\n",
    "max_seconds_in_bucket = 600  # range 0-600 (seconds) for each observation\n",
    "\n",
    "buckets = np.linspace(0, max_seconds_in_bucket, buckets_count, dtype=int) #  List used for binning (used as a name:lower bound of group) \n",
    "buckets = buckets[:-1]\n",
    "\n",
    "output_dir = 'run_results/'\n",
    "test_id = '2nd_attempt' + str(buckets_count)\n",
    "test_dir = output_dir+test_id+\"/\"\n",
    "\n",
    "# Number of columns that should have a value (not nan)\n",
    "nan_threshold = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43eddd42",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96a2f705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wap of 1st entry in book\n",
    "def calc_wap1(df):\n",
    "    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1']+ df['ask_size1'])\n",
    "    return wap\n",
    "\n",
    "# wap of 2nd entry in book\n",
    "def calc_wap2(df):\n",
    "    wap = (df['bid_price2'] * df['bid_size2'] + df['ask_price2'] * df['ask_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
    "    return wap\n",
    "\n",
    "# Function to calculate the log of the return\n",
    "# logb(x / y) = logb(x) - logb(y)\n",
    "def log_return(series):\n",
    "    return np.log(series).diff()\n",
    "\n",
    "def realized_volatility(series):\n",
    "    return np.sqrt(np.sum(series**2))\n",
    "\n",
    "def unique_count(series):\n",
    "    return len(np.unique(series))\n",
    "\n",
    "def read_main_csvs():\n",
    "    train = pd.read_csv(data_dir + 'train.csv')\n",
    "    test = pd.read_csv(data_dir + 'test.csv')\n",
    "    # Create row_id to merge data\n",
    "    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n",
    "    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n",
    "    print(f'The training set has {train.shape[0]} rows')\n",
    "    return train, test\n",
    "\n",
    "def calc_model_importance(model, feature_names=None, importance_type='gain'):\n",
    "    importance_df = pd.DataFrame(model.feature_importance(importance_type=importance_type),\n",
    "                                 index=feature_names,\n",
    "                                 columns=['importance']).sort_values('importance')\n",
    "    return importance_df\n",
    "\n",
    "def calc_mean_importance(importance_df_list):\n",
    "    mean_importance = np.mean(\n",
    "        np.array([df['importance'].values for df in importance_df_list]), axis=0)\n",
    "    mean_df = importance_df_list[0].copy()\n",
    "    mean_df['importance'] = mean_importance\n",
    "    return mean_df\n",
    "\n",
    "def plot_importance(importance_fig, filename):\n",
    "    if not os.path.exists(os.path.dirname(test_dir)):\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(test_dir))\n",
    "        except:\n",
    "            print(\"Could not create dir\",filename)\n",
    "    timestamp = datetime.datetime.now().strftime('%m-%d-%H_%M')\n",
    "    importance_fig.figure.savefig(test_dir+filename+timestamp+\"plot\"+\".png\")\n",
    "    \n",
    "    \n",
    "def save_results_to_file(filename,params,rmspe_score):\n",
    "    if not os.path.exists(os.path.dirname(test_dir)):\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(test_dir))\n",
    "        except:\n",
    "            print(\"Could not create dir\")\n",
    "    timestamp = datetime.datetime.now().strftime('%m-%d-%H_%M')\n",
    "    with open(test_dir+filename+timestamp+'.txt', 'w') as f:\n",
    "        f.write(f'The test RMSPE is {rmspe_score} \\n Hyperparams: {params}')\n",
    "\n",
    "\n",
    "# Calculate the root mean squared percentage error\n",
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "# Early stoping, root mean squared percentage error\n",
    "def feval_rmspe(y_pred, lgb_train):\n",
    "    y_true = lgb_train.get_label()\n",
    "    return 'RMSPE', rmspe(y_true, y_pred), False\n",
    "\n",
    "# Create features for order book data (for a given stock file)\n",
    "def create_bucketed_features(dataframe, aggregate_features_map):\n",
    "    features = pd.DataFrame()\n",
    "\n",
    "    for bucket_start_time in buckets:\n",
    "        # Group into time buckets\n",
    "        bucketed_features = dataframe[dataframe['seconds_in_bucket']\n",
    "                                      .between(bucket_start_time, bucket_start_time + buckets[1] - buckets[0])].groupby(['time_id']).agg(aggregate_features_map)\n",
    "        bucketed_features.columns = ['_'.join(col) for col in bucketed_features.columns]\n",
    "        bucketed_features = bucketed_features.add_suffix('_' + str(bucket_start_time))\n",
    "        if bucket_start_time == 0:\n",
    "            features = bucketed_features\n",
    "        else:\n",
    "            features = features.merge(bucketed_features, how = 'outer', left_on = 'time_id', right_on = 'time_id')\n",
    "    \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f328d7",
   "metadata": {},
   "source": [
    "### 2nd Attempt feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03a187ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_order_book_in_time_buckets(file_path):\n",
    "    dataframe = pd.read_parquet(file_path)\n",
    "    \n",
    "    dataframe['wap2'] = calc_wap1(dataframe)\n",
    "    dataframe['price_diff'] = abs(dataframe['ask_size2'] - dataframe['bid_price2'])\n",
    "  \n",
    "    aggregate_features_map = {\n",
    "        'wap2': [np.mean],\n",
    "        'price_diff': [np.mean],\n",
    "        \n",
    "        'bid_price1': [np.mean],\n",
    "        'ask_price1': [np.mean],\n",
    "        \n",
    "        'bid_price2': [np.mean],\n",
    "        'ask_price2': [np.mean],\n",
    "        \n",
    "        'bid_size1': [np.mean],\n",
    "        'ask_size1': [np.mean],\n",
    "        \n",
    "        'bid_size2': [np.mean],\n",
    "        'ask_size2': [np.mean],\n",
    "    }\n",
    "    \n",
    "        \n",
    "    features = create_bucketed_features(dataframe,aggregate_features_map)\n",
    "    features = features.reset_index(level=0)\n",
    "    # Create row_id to merge\n",
    "    stock_id = file_path.split('=')[1]\n",
    "    features['row_id'] = features['time_id'].apply(lambda x: f'{stock_id}-{x}')\n",
    "    features.drop(['time_id'], axis = 1, inplace = True)\n",
    "    return features\n",
    "\n",
    "# Split into time buckets within a time_id\n",
    "def group_trade_book_in_time_buckets(file_path):\n",
    "    dataframe = pd.read_parquet(file_path)\n",
    "    \n",
    "    aggregate_features_map = {\n",
    "        'price':[np.mean, np.max, np.min],\n",
    "        'size':[np.mean, np.max, np.min],\n",
    "        'order_count':[np.mean, np.max, np.min],\n",
    "    }\n",
    "\n",
    "\n",
    "    features = create_bucketed_features(dataframe,aggregate_features_map)\n",
    "    features = features.reset_index(level=0)\n",
    "    features = features.add_prefix('trade_')\n",
    "    stock_id = file_path.split('=')[1]\n",
    "    features['row_id'] = features['trade_time_id'].apply(lambda x:f'{stock_id}-{x}')\n",
    "    features.drop(['trade_time_id'], axis = 1, inplace = True)\n",
    "    return features\n",
    "\n",
    "# Create group stats for all stocks in each time_id\n",
    "def get_group_time_id_stats(dataframe):\n",
    "    group_stats = []\n",
    "\n",
    "    for bucket in buckets:\n",
    "        group_stats.append(f'wap2_mean_{bucket}')\n",
    "        group_stats.append(f'price_diff_mean_{bucket}')\n",
    "\n",
    "\n",
    "\n",
    "    time_id = dataframe.groupby(['time_id'])[group_stats].agg([np.mean])\n",
    "    time_id.columns = ['_'.join(col) for col in time_id.columns]\n",
    "    time_id = time_id.add_suffix('_' + 'time_id_groupstat')\n",
    "    \n",
    "    # Merge result with main dataframe\n",
    "    dataframe = dataframe.merge(time_id, how = 'inner', left_on = ['time_id'], right_on = ['time_id'])\n",
    "    return dataframe\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdef457",
   "metadata": {},
   "source": [
    "### Main Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ee8fc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stock(stock_id,is_training_set):\n",
    "    # Train\n",
    "    if is_training_set:\n",
    "        order_book_path = f\"{data_dir}book_train.parquet/stock_id={str(stock_id)}\"\n",
    "        trade_book_path = f\"{data_dir}trade_train.parquet/stock_id={str(stock_id)}\"\n",
    "    # Test\n",
    "    else:\n",
    "        order_book_path = f\"{data_dir}book_test.parquet/stock_id={str(stock_id)}\"\n",
    "        trade_book_path = f\"{data_dir}trade_test.parquet/stock_id={str(stock_id)}\"\n",
    "    _tmp = None\n",
    "    # Create order and trade book features and merge them\n",
    "\n",
    "    _tmp = pd.merge(group_order_book_in_time_buckets(order_book_path), group_trade_book_in_time_buckets(trade_book_path), on = ['row_id'], how = 'left')\n",
    "    return _tmp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "519b0cd7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training set has 428932 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    4.6s\n",
      "[Parallel(n_jobs=-1)]: Done 112 out of 112 | elapsed:   16.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 10.2 s\n",
      "Wall time: 25.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train, test = read_main_csvs()\n",
    "\n",
    "train_stock_ids = train['stock_id'].unique()\n",
    "dataframes = None\n",
    "# Features engineering & binning for training set\n",
    "dataframes = Parallel(n_jobs = -1, verbose = 1)(delayed(load_stock)(stock_id,is_training_set=True) for stock_id in train_stock_ids)\n",
    "# for stock_id in train_stock_ids:\n",
    "#     dataframes = load_stock(stock_id, is_training_set=True)\n",
    "train_ = pd.concat(dataframes, ignore_index = True)\n",
    "\n",
    "train = train.merge(train_, on = ['row_id'], how = 'left')\n",
    "\n",
    "\n",
    "test_stock_ids = test['stock_id'].unique()\n",
    "print(test_stock_ids)\n",
    "dataframes = None\n",
    "# Features engineering & binning for test set\n",
    "dataframes = Parallel(n_jobs = -1, verbose = 1)(delayed(load_stock)(stock_id,is_training_set=False) for stock_id in test_stock_ids)\n",
    "# for stock_id in test_stock_ids:\n",
    "#     dataframes = load_stock(stock_id, is_training_set=False)\n",
    "test_ = pd.concat(dataframes, ignore_index = True)\n",
    "\n",
    "test = test.merge(test_, on = ['row_id'], how = 'left')\n",
    "\n",
    "# Generate group stats for time_id and stock_id\n",
    " \n",
    "train = get_group_time_id_stats(train)\n",
    "test = get_group_time_id_stats(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2c1fbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gain_importance_list = []\n",
    "split_importance_list = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b376761e",
   "metadata": {},
   "source": [
    "### Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7362261a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "428932"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7ffea44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_id</th>\n",
       "      <th>time_id</th>\n",
       "      <th>target</th>\n",
       "      <th>row_id</th>\n",
       "      <th>wap2_mean_0</th>\n",
       "      <th>price_diff_mean_0</th>\n",
       "      <th>bid_price1_mean_0</th>\n",
       "      <th>ask_price1_mean_0</th>\n",
       "      <th>bid_price2_mean_0</th>\n",
       "      <th>ask_price2_mean_0</th>\n",
       "      <th>...</th>\n",
       "      <th>wap2_mean_225_mean_time_id_groupstat</th>\n",
       "      <th>price_diff_mean_225_mean_time_id_groupstat</th>\n",
       "      <th>wap2_mean_300_mean_time_id_groupstat</th>\n",
       "      <th>price_diff_mean_300_mean_time_id_groupstat</th>\n",
       "      <th>wap2_mean_375_mean_time_id_groupstat</th>\n",
       "      <th>price_diff_mean_375_mean_time_id_groupstat</th>\n",
       "      <th>wap2_mean_450_mean_time_id_groupstat</th>\n",
       "      <th>price_diff_mean_450_mean_time_id_groupstat</th>\n",
       "      <th>wap2_mean_525_mean_time_id_groupstat</th>\n",
       "      <th>price_diff_mean_525_mean_time_id_groupstat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.004136</td>\n",
       "      <td>0-5</td>\n",
       "      <td>1.002237</td>\n",
       "      <td>118.515404</td>\n",
       "      <td>1.001954</td>\n",
       "      <td>1.002829</td>\n",
       "      <td>1.001838</td>\n",
       "      <td>1.002981</td>\n",
       "      <td>...</td>\n",
       "      <td>1.002719</td>\n",
       "      <td>581.660465</td>\n",
       "      <td>1.002687</td>\n",
       "      <td>589.312578</td>\n",
       "      <td>1.002546</td>\n",
       "      <td>549.179006</td>\n",
       "      <td>1.002421</td>\n",
       "      <td>580.797176</td>\n",
       "      <td>1.002107</td>\n",
       "      <td>601.708474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.006340</td>\n",
       "      <td>1-5</td>\n",
       "      <td>1.002095</td>\n",
       "      <td>63.012460</td>\n",
       "      <td>1.001696</td>\n",
       "      <td>1.002398</td>\n",
       "      <td>1.001590</td>\n",
       "      <td>1.002516</td>\n",
       "      <td>...</td>\n",
       "      <td>1.002719</td>\n",
       "      <td>581.660465</td>\n",
       "      <td>1.002687</td>\n",
       "      <td>589.312578</td>\n",
       "      <td>1.002546</td>\n",
       "      <td>549.179006</td>\n",
       "      <td>1.002421</td>\n",
       "      <td>580.797176</td>\n",
       "      <td>1.002107</td>\n",
       "      <td>601.708474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.001848</td>\n",
       "      <td>2-5</td>\n",
       "      <td>1.000654</td>\n",
       "      <td>209.903615</td>\n",
       "      <td>1.000605</td>\n",
       "      <td>1.000736</td>\n",
       "      <td>1.000523</td>\n",
       "      <td>1.000817</td>\n",
       "      <td>...</td>\n",
       "      <td>1.002719</td>\n",
       "      <td>581.660465</td>\n",
       "      <td>1.002687</td>\n",
       "      <td>589.312578</td>\n",
       "      <td>1.002546</td>\n",
       "      <td>549.179006</td>\n",
       "      <td>1.002421</td>\n",
       "      <td>580.797176</td>\n",
       "      <td>1.002107</td>\n",
       "      <td>601.708474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.005300</td>\n",
       "      <td>3-5</td>\n",
       "      <td>0.999835</td>\n",
       "      <td>197.162303</td>\n",
       "      <td>0.999578</td>\n",
       "      <td>1.000168</td>\n",
       "      <td>0.999462</td>\n",
       "      <td>1.000281</td>\n",
       "      <td>...</td>\n",
       "      <td>1.002719</td>\n",
       "      <td>581.660465</td>\n",
       "      <td>1.002687</td>\n",
       "      <td>589.312578</td>\n",
       "      <td>1.002546</td>\n",
       "      <td>549.179006</td>\n",
       "      <td>1.002421</td>\n",
       "      <td>580.797176</td>\n",
       "      <td>1.002107</td>\n",
       "      <td>601.708474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.004468</td>\n",
       "      <td>4-5</td>\n",
       "      <td>1.002056</td>\n",
       "      <td>93.642818</td>\n",
       "      <td>1.001596</td>\n",
       "      <td>1.002420</td>\n",
       "      <td>1.001336</td>\n",
       "      <td>1.002485</td>\n",
       "      <td>...</td>\n",
       "      <td>1.002719</td>\n",
       "      <td>581.660465</td>\n",
       "      <td>1.002687</td>\n",
       "      <td>589.312578</td>\n",
       "      <td>1.002546</td>\n",
       "      <td>549.179006</td>\n",
       "      <td>1.002421</td>\n",
       "      <td>580.797176</td>\n",
       "      <td>1.002107</td>\n",
       "      <td>601.708474</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 172 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   stock_id  time_id    target row_id  wap2_mean_0  price_diff_mean_0  \\\n",
       "0         0        5  0.004136    0-5     1.002237         118.515404   \n",
       "1         1        5  0.006340    1-5     1.002095          63.012460   \n",
       "2         2        5  0.001848    2-5     1.000654         209.903615   \n",
       "3         3        5  0.005300    3-5     0.999835         197.162303   \n",
       "4         4        5  0.004468    4-5     1.002056          93.642818   \n",
       "\n",
       "   bid_price1_mean_0  ask_price1_mean_0  bid_price2_mean_0  ask_price2_mean_0  \\\n",
       "0           1.001954           1.002829           1.001838           1.002981   \n",
       "1           1.001696           1.002398           1.001590           1.002516   \n",
       "2           1.000605           1.000736           1.000523           1.000817   \n",
       "3           0.999578           1.000168           0.999462           1.000281   \n",
       "4           1.001596           1.002420           1.001336           1.002485   \n",
       "\n",
       "   ...  wap2_mean_225_mean_time_id_groupstat  \\\n",
       "0  ...                              1.002719   \n",
       "1  ...                              1.002719   \n",
       "2  ...                              1.002719   \n",
       "3  ...                              1.002719   \n",
       "4  ...                              1.002719   \n",
       "\n",
       "   price_diff_mean_225_mean_time_id_groupstat  \\\n",
       "0                                  581.660465   \n",
       "1                                  581.660465   \n",
       "2                                  581.660465   \n",
       "3                                  581.660465   \n",
       "4                                  581.660465   \n",
       "\n",
       "   wap2_mean_300_mean_time_id_groupstat  \\\n",
       "0                              1.002687   \n",
       "1                              1.002687   \n",
       "2                              1.002687   \n",
       "3                              1.002687   \n",
       "4                              1.002687   \n",
       "\n",
       "   price_diff_mean_300_mean_time_id_groupstat  \\\n",
       "0                                  589.312578   \n",
       "1                                  589.312578   \n",
       "2                                  589.312578   \n",
       "3                                  589.312578   \n",
       "4                                  589.312578   \n",
       "\n",
       "   wap2_mean_375_mean_time_id_groupstat  \\\n",
       "0                              1.002546   \n",
       "1                              1.002546   \n",
       "2                              1.002546   \n",
       "3                              1.002546   \n",
       "4                              1.002546   \n",
       "\n",
       "   price_diff_mean_375_mean_time_id_groupstat  \\\n",
       "0                                  549.179006   \n",
       "1                                  549.179006   \n",
       "2                                  549.179006   \n",
       "3                                  549.179006   \n",
       "4                                  549.179006   \n",
       "\n",
       "   wap2_mean_450_mean_time_id_groupstat  \\\n",
       "0                              1.002421   \n",
       "1                              1.002421   \n",
       "2                              1.002421   \n",
       "3                              1.002421   \n",
       "4                              1.002421   \n",
       "\n",
       "   price_diff_mean_450_mean_time_id_groupstat  \\\n",
       "0                                  580.797176   \n",
       "1                                  580.797176   \n",
       "2                                  580.797176   \n",
       "3                                  580.797176   \n",
       "4                                  580.797176   \n",
       "\n",
       "   wap2_mean_525_mean_time_id_groupstat  \\\n",
       "0                              1.002107   \n",
       "1                              1.002107   \n",
       "2                              1.002107   \n",
       "3                              1.002107   \n",
       "4                              1.002107   \n",
       "\n",
       "   price_diff_mean_525_mean_time_id_groupstat  \n",
       "0                                  601.708474  \n",
       "1                                  601.708474  \n",
       "2                                  601.708474  \n",
       "3                                  601.708474  \n",
       "4                                  601.708474  \n",
       "\n",
       "[5 rows x 172 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5dde96af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(train) - len(train.dropna(thresh=nan_threshold, axis=0)))\n",
    "print(len(test) - len(test.dropna()))\n",
    "\n",
    "# drop rows (axis=0) if more than 3 columns are nan\n",
    "train.dropna(inplace=True, thresh=nan_threshold, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19dced39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_id</th>\n",
       "      <th>time_id</th>\n",
       "      <th>row_id</th>\n",
       "      <th>wap2_mean_0</th>\n",
       "      <th>price_diff_mean_0</th>\n",
       "      <th>bid_price1_mean_0</th>\n",
       "      <th>ask_price1_mean_0</th>\n",
       "      <th>bid_price2_mean_0</th>\n",
       "      <th>ask_price2_mean_0</th>\n",
       "      <th>bid_size1_mean_0</th>\n",
       "      <th>...</th>\n",
       "      <th>wap2_mean_225_mean_time_id_groupstat</th>\n",
       "      <th>price_diff_mean_225_mean_time_id_groupstat</th>\n",
       "      <th>wap2_mean_300_mean_time_id_groupstat</th>\n",
       "      <th>price_diff_mean_300_mean_time_id_groupstat</th>\n",
       "      <th>wap2_mean_375_mean_time_id_groupstat</th>\n",
       "      <th>price_diff_mean_375_mean_time_id_groupstat</th>\n",
       "      <th>wap2_mean_450_mean_time_id_groupstat</th>\n",
       "      <th>price_diff_mean_450_mean_time_id_groupstat</th>\n",
       "      <th>wap2_mean_525_mean_time_id_groupstat</th>\n",
       "      <th>price_diff_mean_525_mean_time_id_groupstat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0-4</td>\n",
       "      <td>1.000405</td>\n",
       "      <td>18.667011</td>\n",
       "      <td>1.000049</td>\n",
       "      <td>1.000606</td>\n",
       "      <td>0.999656</td>\n",
       "      <td>1.000721</td>\n",
       "      <td>157.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>0-32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>0-34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 171 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   stock_id  time_id row_id  wap2_mean_0  price_diff_mean_0  \\\n",
       "0         0        4    0-4     1.000405          18.667011   \n",
       "1         0       32   0-32          NaN                NaN   \n",
       "2         0       34   0-34          NaN                NaN   \n",
       "\n",
       "   bid_price1_mean_0  ask_price1_mean_0  bid_price2_mean_0  ask_price2_mean_0  \\\n",
       "0           1.000049           1.000606           0.999656           1.000721   \n",
       "1                NaN                NaN                NaN                NaN   \n",
       "2                NaN                NaN                NaN                NaN   \n",
       "\n",
       "   bid_size1_mean_0  ...  wap2_mean_225_mean_time_id_groupstat  \\\n",
       "0        157.333333  ...                                   NaN   \n",
       "1               NaN  ...                                   NaN   \n",
       "2               NaN  ...                                   NaN   \n",
       "\n",
       "   price_diff_mean_225_mean_time_id_groupstat  \\\n",
       "0                                         NaN   \n",
       "1                                         NaN   \n",
       "2                                         NaN   \n",
       "\n",
       "   wap2_mean_300_mean_time_id_groupstat  \\\n",
       "0                                   NaN   \n",
       "1                                   NaN   \n",
       "2                                   NaN   \n",
       "\n",
       "   price_diff_mean_300_mean_time_id_groupstat  \\\n",
       "0                                         NaN   \n",
       "1                                         NaN   \n",
       "2                                         NaN   \n",
       "\n",
       "   wap2_mean_375_mean_time_id_groupstat  \\\n",
       "0                                   NaN   \n",
       "1                                   NaN   \n",
       "2                                   NaN   \n",
       "\n",
       "   price_diff_mean_375_mean_time_id_groupstat  \\\n",
       "0                                         NaN   \n",
       "1                                         NaN   \n",
       "2                                         NaN   \n",
       "\n",
       "   wap2_mean_450_mean_time_id_groupstat  \\\n",
       "0                                   NaN   \n",
       "1                                   NaN   \n",
       "2                                   NaN   \n",
       "\n",
       "   price_diff_mean_450_mean_time_id_groupstat  \\\n",
       "0                                         NaN   \n",
       "1                                         NaN   \n",
       "2                                         NaN   \n",
       "\n",
       "   wap2_mean_525_mean_time_id_groupstat  \\\n",
       "0                                   NaN   \n",
       "1                                   NaN   \n",
       "2                                   NaN   \n",
       "\n",
       "   price_diff_mean_525_mean_time_id_groupstat  \n",
       "0                                         NaN  \n",
       "1                                         NaN  \n",
       "2                                         NaN  \n",
       "\n",
       "[3 rows x 171 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "656df95d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "428932\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['stock_id', 'time_id', 'target', 'row_id', 'wap2_mean_0',\n",
       "       'price_diff_mean_0', 'bid_price1_mean_0', 'ask_price1_mean_0',\n",
       "       'bid_price2_mean_0', 'ask_price2_mean_0',\n",
       "       ...\n",
       "       'wap2_mean_225_mean_time_id_groupstat',\n",
       "       'price_diff_mean_225_mean_time_id_groupstat',\n",
       "       'wap2_mean_300_mean_time_id_groupstat',\n",
       "       'price_diff_mean_300_mean_time_id_groupstat',\n",
       "       'wap2_mean_375_mean_time_id_groupstat',\n",
       "       'price_diff_mean_375_mean_time_id_groupstat',\n",
       "       'wap2_mean_450_mean_time_id_groupstat',\n",
       "       'price_diff_mean_450_mean_time_id_groupstat',\n",
       "       'wap2_mean_525_mean_time_id_groupstat',\n",
       "       'price_diff_mean_525_mean_time_id_groupstat'],\n",
       "      dtype='object', length=172)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(train.index))\n",
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5435018d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(['time_id'], axis = 1)\n",
    "test = test.drop(['time_id'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7e84f2",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e6895db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(train, test):\n",
    "    # Hyperparammeters (just basic)\n",
    "    hyperparameters = {\n",
    "        'objective': 'rmse',  \n",
    "        'boosting_type': 'gbdt',\n",
    "        #       'boosting_type': 'dart', # Try to solve over-specialization problem in gbdt\n",
    "        #       'boosting_type': 'goss', # Goss provides a new sampling method for GBDT by separating those instances with larger gradients.\n",
    "        'num_iterations': 5678, # Num_iterations specifies the number of boosting iterations (trees to build). The more trees you build the more accurate your model can be at the cost of:\n",
    "        'n_estimators': 20000,#controls the number of decision trees \n",
    "        'min_data_in_leaf': 9,\n",
    "        'max_bin': 75, # If you define max_bin 255 that means we can have a maximum of 255 unique values per feature. Then Small max_bin causes faster speed and large value improves accuracy.\n",
    "        'num_leaves': 27,\n",
    "        'max_depth': 9, # This parameter control max depth of each trained tree and will have impact on:       The best value for the num_leaves parameter     Model Performance     Training Time  Pay attention If you use a large value of max_depth, your model will likely be over fit  to the train set\n",
    "        'n_jobs': -1,\n",
    "        'learning_rate': 0.095, # learning_rate > 0.0Typical: 0.05.\n",
    "        'feature_fraction': 0.9, # or sub_feature deals with column sampling, LightGBM will randomly select a subset of features on each iteration (tree)\n",
    "        'bagging_fraction': 0.9, # you can specify the percentage of rows used per tree building iteration.\n",
    "        'verbose': -1,\n",
    "        # 'is_unbalance': True #One of the problems you may face in the binary classification problems is how to deal with the unbalanced datasets. Obviously, you need to balance positive/negative samples but how exactly can you do that in lightgbm      \n",
    "    }\n",
    "    \n",
    "    if use_GPU:\n",
    "        hyperparameters.update(\n",
    "            {'device' : 'gpu',\n",
    "            'gpu_platform_id' : 0,\n",
    "            'gpu_device_id' : 0,}\n",
    "        )\n",
    "     \n",
    "   \n",
    "    # We need to drop row_id as it contains info from both time_id and stock_id and time id should be out of the features | should only be used in predictions\n",
    "    x = train.drop(['row_id', 'target'], axis = 1)\n",
    "    y = train['target']\n",
    "    x_test = test.drop(['row_id'], axis = 1)\n",
    "\n",
    "    x['stock_id'] = x['stock_id'].astype(int)\n",
    "    x_test['stock_id'] = x_test['stock_id'].astype(int)\n",
    "    \n",
    "    test_predictions = np.zeros(x_test.shape[0])\n",
    "    \n",
    "    # Create out of folds array\n",
    "    oof_predictions = np.zeros(x.shape[0])\n",
    "\n",
    "    \n",
    "    kfold = KFold(n_splits = 5, random_state = 66, shuffle = True)\n",
    "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(x)):\n",
    "        print(f'Training fold {fold + 1}')\n",
    "        x_train, x_val = x.iloc[trn_ind], x.iloc[val_ind]\n",
    "        y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n",
    "        # Root mean squared percentage error weights\n",
    "        train_weights = 1 / np.square(y_train)\n",
    "        val_weights = 1 / np.square(y_val)\n",
    "        train_dataset = lgb.Dataset(x_train, y_train, weight = train_weights, categorical_feature = ['stock_id'])\n",
    "        val_dataset = lgb.Dataset(x_val, y_val, weight = val_weights, categorical_feature = ['stock_id'])\n",
    "        model = lgb.train(params = hyperparameters, \n",
    "                          train_set = train_dataset, \n",
    "                          valid_sets = [train_dataset, val_dataset], \n",
    "                          num_boost_round = 10000, \n",
    "                          early_stopping_rounds = 50, \n",
    "                          verbose_eval = 50,\n",
    "                          feval = feval_rmspe)\n",
    "        # Add predictions to the out of folds array\n",
    "        oof_predictions[val_ind] = model.predict(x_val)\n",
    "        # Predict the test set\n",
    "        test_predictions += model.predict(x_test) / 5\n",
    "    \n",
    "    importance_plot = lgb.plot_importance(model,max_num_features=7,figsize=(32,15))\n",
    "    plot_importance(importance_plot, \"feature_importance_plot\")\n",
    "    rmspe_score = rmspe(y, oof_predictions)\n",
    "    print(f'The out of folds RMSPE is {rmspe_score}')\n",
    "\n",
    "    return test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ebef421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's rmse: 0.000629493\ttraining's RMSPE: 0.291238\tvalid_1's rmse: 0.000635981\tvalid_1's RMSPE: 0.294741\n",
      "[100]\ttraining's rmse: 0.000578685\ttraining's RMSPE: 0.267732\tvalid_1's rmse: 0.000591488\tvalid_1's RMSPE: 0.274121\n",
      "[150]\ttraining's rmse: 0.000550692\ttraining's RMSPE: 0.25478\tvalid_1's rmse: 0.000568727\tvalid_1's RMSPE: 0.263573\n",
      "[200]\ttraining's rmse: 0.00053082\ttraining's RMSPE: 0.245586\tvalid_1's rmse: 0.000553537\tvalid_1's RMSPE: 0.256533\n",
      "[250]\ttraining's rmse: 0.00051526\ttraining's RMSPE: 0.238388\tvalid_1's rmse: 0.000543893\tvalid_1's RMSPE: 0.252064\n",
      "[300]\ttraining's rmse: 0.000502706\ttraining's RMSPE: 0.23258\tvalid_1's rmse: 0.000536489\tvalid_1's RMSPE: 0.248632\n",
      "[350]\ttraining's rmse: 0.00049089\ttraining's RMSPE: 0.227113\tvalid_1's rmse: 0.000528851\tvalid_1's RMSPE: 0.245093\n",
      "[400]\ttraining's rmse: 0.00048149\ttraining's RMSPE: 0.222764\tvalid_1's rmse: 0.000523815\tvalid_1's RMSPE: 0.242758\n",
      "[450]\ttraining's rmse: 0.000473334\ttraining's RMSPE: 0.21899\tvalid_1's rmse: 0.000519338\tvalid_1's RMSPE: 0.240684\n",
      "[500]\ttraining's rmse: 0.000466038\ttraining's RMSPE: 0.215615\tvalid_1's rmse: 0.00051566\tvalid_1's RMSPE: 0.238979\n",
      "[550]\ttraining's rmse: 0.000459199\ttraining's RMSPE: 0.212451\tvalid_1's rmse: 0.000512026\tvalid_1's RMSPE: 0.237295\n",
      "[600]\ttraining's rmse: 0.000452913\ttraining's RMSPE: 0.209543\tvalid_1's rmse: 0.000509151\tvalid_1's RMSPE: 0.235963\n",
      "[650]\ttraining's rmse: 0.000447406\ttraining's RMSPE: 0.206994\tvalid_1's rmse: 0.000507077\tvalid_1's RMSPE: 0.235001\n",
      "[700]\ttraining's rmse: 0.000443018\ttraining's RMSPE: 0.204965\tvalid_1's rmse: 0.000505917\tvalid_1's RMSPE: 0.234464\n",
      "[750]\ttraining's rmse: 0.00043793\ttraining's RMSPE: 0.202611\tvalid_1's rmse: 0.00050378\tvalid_1's RMSPE: 0.233473\n",
      "[800]\ttraining's rmse: 0.000432406\ttraining's RMSPE: 0.200055\tvalid_1's rmse: 0.000501119\tvalid_1's RMSPE: 0.23224\n",
      "[850]\ttraining's rmse: 0.000428344\ttraining's RMSPE: 0.198176\tvalid_1's rmse: 0.000500058\tvalid_1's RMSPE: 0.231748\n",
      "[900]\ttraining's rmse: 0.000424506\ttraining's RMSPE: 0.1964\tvalid_1's rmse: 0.000498878\tvalid_1's RMSPE: 0.231202\n",
      "[950]\ttraining's rmse: 0.0004199\ttraining's RMSPE: 0.194269\tvalid_1's rmse: 0.000497267\tvalid_1's RMSPE: 0.230455\n",
      "[1000]\ttraining's rmse: 0.000416345\ttraining's RMSPE: 0.192624\tvalid_1's rmse: 0.000496682\tvalid_1's RMSPE: 0.230184\n",
      "[1050]\ttraining's rmse: 0.000412866\ttraining's RMSPE: 0.191014\tvalid_1's rmse: 0.000495624\tvalid_1's RMSPE: 0.229693\n",
      "[1100]\ttraining's rmse: 0.000409858\ttraining's RMSPE: 0.189623\tvalid_1's rmse: 0.000496038\tvalid_1's RMSPE: 0.229885\n",
      "Early stopping, best iteration is:\n",
      "[1092]\ttraining's rmse: 0.000410321\ttraining's RMSPE: 0.189837\tvalid_1's rmse: 0.000495536\tvalid_1's RMSPE: 0.229653\n",
      "Training fold 2\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's rmse: 0.000627724\ttraining's RMSPE: 0.290724\tvalid_1's rmse: 0.000655103\tvalid_1's RMSPE: 0.30233\n",
      "[100]\ttraining's rmse: 0.000577428\ttraining's RMSPE: 0.26743\tvalid_1's rmse: 0.000610687\tvalid_1's RMSPE: 0.281832\n",
      "[150]\ttraining's rmse: 0.000549687\ttraining's RMSPE: 0.254582\tvalid_1's rmse: 0.000586689\tvalid_1's RMSPE: 0.270757\n",
      "[200]\ttraining's rmse: 0.000529228\ttraining's RMSPE: 0.245107\tvalid_1's rmse: 0.000570026\tvalid_1's RMSPE: 0.263067\n",
      "[250]\ttraining's rmse: 0.000514292\ttraining's RMSPE: 0.238189\tvalid_1's rmse: 0.00055898\tvalid_1's RMSPE: 0.25797\n",
      "[300]\ttraining's rmse: 0.000501797\ttraining's RMSPE: 0.232402\tvalid_1's rmse: 0.000550585\tvalid_1's RMSPE: 0.254095\n",
      "[350]\ttraining's rmse: 0.000490693\ttraining's RMSPE: 0.22726\tvalid_1's rmse: 0.000541359\tvalid_1's RMSPE: 0.249837\n",
      "[400]\ttraining's rmse: 0.000481674\ttraining's RMSPE: 0.223083\tvalid_1's rmse: 0.000536038\tvalid_1's RMSPE: 0.247382\n",
      "[450]\ttraining's rmse: 0.000473797\ttraining's RMSPE: 0.219434\tvalid_1's rmse: 0.000532409\tvalid_1's RMSPE: 0.245707\n",
      "[500]\ttraining's rmse: 0.000465251\ttraining's RMSPE: 0.215477\tvalid_1's rmse: 0.000527103\tvalid_1's RMSPE: 0.243258\n",
      "[550]\ttraining's rmse: 0.00045914\ttraining's RMSPE: 0.212646\tvalid_1's rmse: 0.000524438\tvalid_1's RMSPE: 0.242028\n",
      "[600]\ttraining's rmse: 0.000453251\ttraining's RMSPE: 0.209919\tvalid_1's rmse: 0.000521975\tvalid_1's RMSPE: 0.240892\n",
      "[650]\ttraining's rmse: 0.000447562\ttraining's RMSPE: 0.207284\tvalid_1's rmse: 0.000519549\tvalid_1's RMSPE: 0.239772\n",
      "[700]\ttraining's rmse: 0.000442444\ttraining's RMSPE: 0.204914\tvalid_1's rmse: 0.000518194\tvalid_1's RMSPE: 0.239147\n",
      "[750]\ttraining's rmse: 0.000438199\ttraining's RMSPE: 0.202947\tvalid_1's rmse: 0.000516972\tvalid_1's RMSPE: 0.238583\n",
      "[800]\ttraining's rmse: 0.000433539\ttraining's RMSPE: 0.20079\tvalid_1's rmse: 0.000515094\tvalid_1's RMSPE: 0.237716\n",
      "[850]\ttraining's rmse: 0.000429053\ttraining's RMSPE: 0.198712\tvalid_1's rmse: 0.000513144\tvalid_1's RMSPE: 0.236816\n",
      "[900]\ttraining's rmse: 0.000424621\ttraining's RMSPE: 0.196659\tvalid_1's rmse: 0.000510915\tvalid_1's RMSPE: 0.235787\n",
      "[950]\ttraining's rmse: 0.000420954\ttraining's RMSPE: 0.194961\tvalid_1's rmse: 0.000509788\tvalid_1's RMSPE: 0.235268\n",
      "[1000]\ttraining's rmse: 0.000417641\ttraining's RMSPE: 0.193426\tvalid_1's rmse: 0.000508873\tvalid_1's RMSPE: 0.234845\n",
      "[1050]\ttraining's rmse: 0.000414546\ttraining's RMSPE: 0.191993\tvalid_1's rmse: 0.000508571\tvalid_1's RMSPE: 0.234706\n",
      "[1100]\ttraining's rmse: 0.000410938\ttraining's RMSPE: 0.190322\tvalid_1's rmse: 0.000507313\tvalid_1's RMSPE: 0.234125\n",
      "[1150]\ttraining's rmse: 0.000408104\ttraining's RMSPE: 0.189009\tvalid_1's rmse: 0.00050673\tvalid_1's RMSPE: 0.233856\n",
      "Early stopping, best iteration is:\n",
      "[1138]\ttraining's rmse: 0.000408658\ttraining's RMSPE: 0.189266\tvalid_1's rmse: 0.000506701\tvalid_1's RMSPE: 0.233843\n",
      "Training fold 3\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's rmse: 0.000629181\ttraining's RMSPE: 0.291211\tvalid_1's rmse: 0.00064198\tvalid_1's RMSPE: 0.297042\n",
      "[100]\ttraining's rmse: 0.000578728\ttraining's RMSPE: 0.26786\tvalid_1's rmse: 0.000601807\tvalid_1's RMSPE: 0.278454\n",
      "[150]\ttraining's rmse: 0.000550621\ttraining's RMSPE: 0.25485\tvalid_1's rmse: 0.000579809\tvalid_1's RMSPE: 0.268275\n",
      "[200]\ttraining's rmse: 0.000530385\ttraining's RMSPE: 0.245484\tvalid_1's rmse: 0.000564998\tvalid_1's RMSPE: 0.261423\n",
      "[250]\ttraining's rmse: 0.000514892\ttraining's RMSPE: 0.238313\tvalid_1's rmse: 0.000554001\tvalid_1's RMSPE: 0.256334\n",
      "[300]\ttraining's rmse: 0.000502109\ttraining's RMSPE: 0.232397\tvalid_1's rmse: 0.000545577\tvalid_1's RMSPE: 0.252436\n",
      "[350]\ttraining's rmse: 0.000490563\ttraining's RMSPE: 0.227053\tvalid_1's rmse: 0.000537291\tvalid_1's RMSPE: 0.248603\n",
      "[400]\ttraining's rmse: 0.000481961\ttraining's RMSPE: 0.223072\tvalid_1's rmse: 0.000532945\tvalid_1's RMSPE: 0.246592\n",
      "[450]\ttraining's rmse: 0.000473264\ttraining's RMSPE: 0.219046\tvalid_1's rmse: 0.000528002\tvalid_1's RMSPE: 0.244305\n",
      "[500]\ttraining's rmse: 0.0004664\ttraining's RMSPE: 0.215869\tvalid_1's rmse: 0.000526056\tvalid_1's RMSPE: 0.243404\n",
      "[550]\ttraining's rmse: 0.000459771\ttraining's RMSPE: 0.212801\tvalid_1's rmse: 0.000522281\tvalid_1's RMSPE: 0.241657\n",
      "[600]\ttraining's rmse: 0.000453316\ttraining's RMSPE: 0.209813\tvalid_1's rmse: 0.000519561\tvalid_1's RMSPE: 0.240399\n",
      "[650]\ttraining's rmse: 0.000447418\ttraining's RMSPE: 0.207084\tvalid_1's rmse: 0.000516839\tvalid_1's RMSPE: 0.239139\n",
      "[700]\ttraining's rmse: 0.000441976\ttraining's RMSPE: 0.204565\tvalid_1's rmse: 0.000513685\tvalid_1's RMSPE: 0.23768\n",
      "[750]\ttraining's rmse: 0.000436996\ttraining's RMSPE: 0.20226\tvalid_1's rmse: 0.000511064\tvalid_1's RMSPE: 0.236467\n",
      "[800]\ttraining's rmse: 0.000432582\ttraining's RMSPE: 0.200217\tvalid_1's rmse: 0.000509087\tvalid_1's RMSPE: 0.235553\n",
      "[850]\ttraining's rmse: 0.00042863\ttraining's RMSPE: 0.198388\tvalid_1's rmse: 0.000507783\tvalid_1's RMSPE: 0.234949\n",
      "[900]\ttraining's rmse: 0.000424599\ttraining's RMSPE: 0.196522\tvalid_1's rmse: 0.000506119\tvalid_1's RMSPE: 0.23418\n",
      "[950]\ttraining's rmse: 0.000420669\ttraining's RMSPE: 0.194703\tvalid_1's rmse: 0.000505018\tvalid_1's RMSPE: 0.23367\n",
      "[1000]\ttraining's rmse: 0.000417093\ttraining's RMSPE: 0.193048\tvalid_1's rmse: 0.000504242\tvalid_1's RMSPE: 0.233311\n",
      "[1050]\ttraining's rmse: 0.000413354\ttraining's RMSPE: 0.191318\tvalid_1's rmse: 0.000502775\tvalid_1's RMSPE: 0.232632\n",
      "[1100]\ttraining's rmse: 0.000409532\ttraining's RMSPE: 0.189548\tvalid_1's rmse: 0.000501571\tvalid_1's RMSPE: 0.232075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1150]\ttraining's rmse: 0.000406239\ttraining's RMSPE: 0.188024\tvalid_1's rmse: 0.000500502\tvalid_1's RMSPE: 0.23158\n",
      "[1200]\ttraining's rmse: 0.000403416\ttraining's RMSPE: 0.186718\tvalid_1's rmse: 0.000500169\tvalid_1's RMSPE: 0.231426\n",
      "Early stopping, best iteration is:\n",
      "[1174]\ttraining's rmse: 0.000404752\ttraining's RMSPE: 0.187336\tvalid_1's rmse: 0.00049996\tvalid_1's RMSPE: 0.231329\n",
      "Training fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Traing and evaluating\n",
    "test_predictions = train_and_evaluate(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00eb582c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0903fc73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5ba496",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b4e010",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296d3d1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db98864a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [17], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Save test predictions for submission\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtest_predictions\u001b[49m\n\u001b[0;32m      3\u001b[0m test[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrow_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubmission.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_predictions' is not defined"
     ]
    }
   ],
   "source": [
    "# Save test predictions for submission\n",
    "test['target'] = test_predictions\n",
    "test[['row_id', 'target']].to_csv('submission.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf1efb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed96d1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29df12d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f46f424",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "b348351ff64ef7b8239b01d02f7050e1a14691eae76471a4c9b3066032a9fe46"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
